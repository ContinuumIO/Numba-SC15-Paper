% THIS IS SIGPROC-SP.TEX - VERSION 3.1
% WORKS WITH V3.2SP OF ACM_PROC_ARTICLE-SP.CLS
% APRIL 2009
%
% It is an example file showing how to use the 'acm_proc_article-sp.cls' V3.2SP
% LaTeX2e document class file for Conference Proceedings submissions.
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V3.2SP) *DOES NOT* produce:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) Page numbering
% ---------------------------------------------------------------------------------------------------------------
% It is an example which *does* use the .bib file (from which the .bbl file
% is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission,
% you need to 'insert'  your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% Questions regarding SIGS should be sent to
% Adrienne Griscti ---> griscti@acm.org
%
% Questions/suggestions regarding the guidelines, .tex and .cls files, etc. to
% Gerald Murray ---> murray@hq.acm.org
%
% For tracking purposes - this is V3.1SP - APRIL 2009

\documentclass{acm_proc_article-sp}

\begin{document}

\title{Numba: A LLVM-based Python JIT Compiler}
% \subtitle{[Extended Abstract]
%\titlenote{A full version of this paper is available as
%\textit{Author's Guide to Preparing ACM SIG Proceedings Using
%\LaTeX$2_\epsilon$\ and BibTeX} at
%\texttt{www.acm.org/eaddress.htm}}}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{3}

%  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Siu Kwan Lam\\
    \affaddr{Continuum Analytics}\\
    \affaddr{Austin, Texas}\\
    \email{slam@continuum.io}
% 2nd. author
\alignauthor
Antoine Pitrou\\
    \affaddr{Continuum Analytics}\\
    \affaddr{Austin, Texas}\\
    \email{apitrou@continuum.io}
%3rd. author
\alignauthor
Stanley Seibert\\
    \affaddr{Continuum Analytics}\\
    \affaddr{Austin, Texas}\\
    \email{sseibert@continuum.io}
}

\date{3 Sept 2015}

% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}
Dynamic, interpreted languages, like Python, are attractive for
domain-experts and scientists experimenting with new ideas.  However,
the performance of the interpreter is often a barrier when scaling to
larger data sets.  This paper presents a just-in-time compiler for
Python that focuses in scientific and array-oriented
computing. Starting with the simple syntax of Python, Numba compiles a
subset of the language into efficient machine code that is comparable
in performance to a traditional compiled language. In addition, we
share our experience in building a JIT compiler using LLVM\cite{LLVM:CGO04}.
\end{abstract}

% A category with the (minimum) three required fields
\category{D.3.4}{Programming Languages}{Processors}[Compilers, Optimization]
%A category including the fourth, optional field follows...
%\category{I.2.5}{Porgramming Languages and Software}{Metrics}[complexity measures, performance measures]

\terms{Languages, Performance}

\keywords{LLVM, Python, Compiler} % NOT required for Proceedings

\section{Introduction}

Python is a dynamically typed, interpreted language.  It is regarded
as a high productivity language due to its simple syntax, flexible
semantics and the large number of libraries.  As a result, it has
gained popularity in the scientific computing
community\cite{oliphant:python}, which has driven the demand for high
performance tools.  Among many performance-focused libraries in
Python, NumPy\cite{vanderwalt:numpy} is one of the important
scientific libraries in the Python ecosystem as it provides a
multi-dimensional array (ndarray) object that has become the
foundation of efficient numeric computation in Python.

A ndarray is a homogeneously-typed data memory buffer.  Data can be
stored in either C contiguous, FORTRAN contiguous or have arbitrary
strides at each dimension.  This allows binding to high performance
computing libraries that are traditionally written in C or FORTRAN,
such as BLAS and LAPACK. The ndarray object also supports array
expressions: using Python operators on ndarrays will trigger
element-wise operations that are implemented in C efficiently.  This
provides a large speedup over interpreted Python, where looping over
the elements in Python is inefficient due to the multiple layers of
indirection inside interpreted code.

Numba\footnote{\url{http://numba.pydata.org/}} was initially
developed to optimize the inefficient use-cases of NumPy.  The layers
of indirection for ndarray indexing in the interpreter are translated
into direct loads and stores from pointers.  Before Numba, NumPy users
had to write Python C extensions (or use hybrid languages like Cython
\cite{behnel:cython}) to implement any custom computation in an
efficient way.  This process can be error-prone due to the difficulty
of manually managing the reference counts of Python objects, and
generally requires a lot of boilerplate code even for simple use
cases.  Numba lets users annotate a compute-intensive Python function
for compilation without rewriting the code in a low-level language.

\section{A JIT for Numeric Python}

Numba is a function-at-a-time Just-in-Time (JIT) compiler for CPython
\footnote{\url{https://www.python.org/}}, the most popular
implementation of Python which, as the name suggests, is written in C.
Numba is implemented as a library that can be loaded by programs
running in the CPython interpreter and does not replace the interpreter
itself.  Its initial focus is to target a Python subset that makes
heavy use of ndarrays and numeric scalars in loops so that users no
longer need to rewrite their Python code in low-level languages for
better performance.  The ndarray provides important dimensionality,
type and data layout information that allows Numba to generate
specialized loops for arrays in machine code.  By knowing the native
in-memory structure of ndarray objects, Numba bypasses unnecessary
indirection for accessing data in ndarrays.  The basic structure of
the ndarray consists of the data pointer to the base of the memory
buffer and two integer arrays describing the dimensionality (aka
shape) and the strides between elements along each dimension. Numba
can directly access these fields for calculating the offset of an
element given an index value.  As a result, it can generate efficient
loops that index into ndarrays with performance similar to equivalent
code written in compiled language (see Table \ref{table:speedup}). As
Numba continues to develop, the supported subset of the Python
language and standard library data types is expanding.  For example,
Numba can already generate efficient code for high-level Python/NumPy
features such as generators and array expressions.

\begin{table}
\centering
\caption{Numba (version 0.20) and pure C speedups over CPython (version 3.4) for a naive implementation of matrix multiplication.}
\label{table:speedup}
\begin{tabular}{|c|c|c|} \hline
\textbf{Matrix Size} & \textbf{Numba} & \textbf{C}\\ \hline
64 x 64 & 463x & 453x \\ \hline
128 x 128 & 454x & 407x \\ \hline
256 x 256 & 280x & 263x \\ \hline
512 x 512 & 276x & 268x \\ \hline
\end{tabular}
\end{table}

% jit speedup [ 463.5193133   454.40414508  280.73770492  276.26262626]
% c   speedup [ 453.78151261  407.90697674  263.46153846  268.1372549 ]


Unlike most JIT compilers for interpreted languages, Numba does not
perform tracing nor replace the interpreter.  Instead, it relies on
the user actively transforming the Python functions that need
compiling at runtime.  In Python, this is done by applying a decorator
to the function.  The decorator replaces the original Python function
with a special object that just-in-time compiles the function when it
is first called with a new type signature.  To relieve user from the
burden of explicit type annotation, Numba inspects the types of the
arguments and performs local type inference on the function.  As a
result, the compiler can have accurate type information for each value
in the function without tracing the execution.

The semantics of the Numba-compiled code differ slightly from code
interpreted by CPython. Numba only supports a subset of the language
and violates some semantics of Python in a few specific areas that are
generally acceptable for high performance numeric use cases.  The
compiler does not support the ``big integer'' representation used in
Python to represent integers of arbitrary size.  Instead, it is
limited to fixed width integer types up to 64-bits in size, with
semantics similar to many C platforms.  Polymorphic types are
not supported during execution, requiring the compiler to be able to
resolve all polymorphism at compile time (when the function is
called).  Implicit coercion occurs when a type is assigned to a
variable of a different type. In practice, these restrictions and
deviations from Python semantics rarely affect the user because the
ndarray object already has similar limitations. On the other hand,
these limitations allow for aggressive optimization by the compiler.
As Numba continues to develop, we aim to remove these limitations when
practical, but leave an option for user permit the violation of Python
semantics for optimization reason.

One novel feature of Numba is its support for targeting different
hardware.  It currently provides an NVIDIA CUDA\cite{CUDA} back-end using the
NVVM\footnote{\url{http://docs.nvidia.com/cuda/nvvm-ir-spec/}}
library, and an AMD HSA\cite{HSA} back-end is also available on APU by using
HLC\footnote{\url{https://github.com/HSAFoundation/HLC-HSAIL-Development-LLVM}}.
Both NVVM and HLC are vendor-specific version of LLVM with additional
support for their hardware.  Rather than attempt to create a portable
execution model supported by all targets, Numba directly exposes the
execution model of each GPGPU architecture to Python.  This allows the
user to tailor their code to the specific features and performance
characteristics of each architecture.  Numba provides access to
platform specific operations, such as thread barriers and atomic
operations on GPGPU targets.

\section{Other Just-in-Time compilers for Python}

There are or were several other efforts to build JIT compilers for
Python.  Closest to Numba perhaps was
Psyco\footnote{\url{http://psyco.sourceforge.net/}}, an opt-in
compiler layered on the CPython interpreter.  Psyco featured its own
x86-only code generator and was discontinued by its author, Armin
Rigo, because its architecture made maintenance and evolution
difficult. \cite{rigo2004representation}

Another related effort was
Unladen Swallow\footnote{\url{https://code.google.com/p/unladen-swallow/}},
a project sponsored by Google to produce an evolution of the CPython interpreter
augmented with a LLVM-based Just-in-Time compiler.
The objective of Unladen Swallow was to support the full range of existing
Python code, and to make the generated code progressively faster by adding
``proven" optimizations.  The project was discontinued before it got
significant results for reasons which were explained by one of its authors.
\cite{kleckner:unladen_swallow_post_mortem}

Other Python JIT compilers have avoided building on CPython, either
re-using a significant portion of the CPython runtime or building
their own.  Of the two main active projects today, PyPy
\cite{pypy:pypy} has built its own compilation infrastructure from
scratch, whereas the other
Pyston\footnote{\url{https://github.com/dropbox/pyston}} is using
LLVM.  Both claim or intend to support the full range of features
Python offers.

Compared to those, Numba is much more opportunistic and opinionated.
It is opportunistic because of the focus on a narrow subset of Python's use-cases
from which it aims to extract extremely good performance, comparable to what C
or FORTRAN code would achieve (or even better, when running on the GPU).
It is opinionated because it prefers and promotes the style of array-oriented
programming via the use of NumPy ndarrays.
Numba is also designed to support different execution models, allowing both
CPU and GPU code to be handled by a common compiler framework.


\section{Implementation}

Numba relies on user annotation by using a Python decorator (@jit) on functions.
The decorator substitute the function object with a special object that triggers
the compilation when called. When a decorated function is called,
the call arguments are inspected. If the set of argument types have not appeared
before, Numba compile will compile a specialized version of the function for the
given types. Otherwise, the previously compiled version is called.

The following example illustrate the usage of the @jit decorator on a function:


\begin{lstlisting}
from numba import jit

@jit
def foo(a):
  return a + a
\end{lstlisting}

The compilation starts by converting the Python bytecode into the an
intermediate representation (IR) on which the type inference is
performed. If the type of each value in the IR can be inferred, the IR
is lowered to LLVM, which then emits the final machine code.  We call
this the \textit{nopython mode} because all operations can be lowered
into efficient machine code without relying on the Python runtime for
any operation.  A \textit{nopython mode} function can be executed
without the \textit{global interpreter lock} (GIL) and is able to run
in parallel threads.  If Numba cannot determine the type of one of the
values in the IR, it assumes to all values in the function to be a
Python object. In such situations, Numba must use the Python C-API and
rely on the Python runtime for the execution. The generated code would
be equivalent to unrolling the interpreter loop; thus, removing the
interpreter overhead.  This compilation mode, called \textit{object
  mode}, serves as a fallback if Numba cannot infer the type.

\subsection{Bytecode}

When calling a function during normal CPython execution, the
interpreter creates a new frame for the function and executes the
function bytecode.  The bytecode is an instruction stream similar to
x86 assembly.  Instructions are variable length. Branches are encoded
as absolute or relative jump instructions. Some bytecode instructions
perform multiple tasks.  For instance, the
JUMP\_IF\_TRUE\_OR\_POP jump to the target address if the
value on the top-of-stack (TOS) evaluates to true; otherwise, pop the
TOS value.  Another complex instruction is the FOR\_ITER instruction.
It is used in the encoding of the for-loop construct.  This
instruction asks for the next value of the iterator at TOS.  If the
iterator is exhausted, it pops the stack and jumps to end of loop
indicated by the operand of the instruction.  Otherwise, the next
value of the iterator is pushed on to the stack and proceed to the
next instruction, which is the first instruction of the loop
body. Both of these instructions have changes the control-flow and
have optional stack-effect. \cite{pythondoc:dis} Instructions like
these are common and they cannot be mapped directly to the low-level
representation used by LLVM IR.  For this reason, the bytecode must
pass through several stages of analysis before translation.

\subsection{Disassembling the bytecode}

Translation of the bytecode to LLVM IR is not trivial due to the
complexity of many common bytecode instructions.  We need to disassemble
the bytecode to recover the basic-blocks and to convert the stack machine into
a register machine to facilitate further analysis and lowering.

The first step of disassembling the bytecode is to recover the control
flow information. The bytecode is scanned for jump targets. The jump
targets indicate the start of the basic-blocks.  Jump instructions
mark the end of basic-blocks. In some cases, this can be a simple
process but, as mentioned in previous section, some bytecode
instructions can perform operations on one of the branch targets.
Heuristics are used to reconstruct the basic-blocks properly. As a
result, our bytecode disassembler must be tailored to the specific
behavior of each CPython version.  Different versions of CPython can
change both the bytecode instruction set and the way certain syntactic
constructs are encoded.

Once all instructions are grouped into basic-blocks. We can simulate stack
operations to assign a virtual register to each value.  Each basic-block is
simulated individually. Therefore, it is possible that the stack is empty when
a pop is encountered. In that case, a \textit{phi} node is inserted.
Its incoming values are later connected to the lingering stack value from the
incoming basic blocks.

\subsection{Internal Representation}

With the control-flow information and the stack-to-register mapping
of the bytecode, the bytecode is translated into an internal representation,
called the Numba IR. The Numba IR is a higher-level representation of the
function logic than the bytecode. It captures the control-flow as basic-blocks
and values in variables.

During the translation to the Numba IR, a notion of scoping is introduced to
minimize the effect of mono-typing for variables. An assignment to a variable
is considered a new variable definition when the definition is visible by all
subsequent basic-blocks. Otherwise, the assignment stores the new value to
the previous definition.  For example:

\begin{lstlisting}
@jit
def foo():
  a = 1
  bar(a)
  a = a + 2.5
  bar(a)
  a = a + 2j
  bar(a)
  return a
\end{lstlisting}

The three assignments to \textit{a} will create three definitions that have
types integer, float and complex, respectively.  As a result, the three
calls to \textit{bar} will be calling different overloaded versions.

\subsection{Type Inference}

Local type inference is applied to the Numba IR at call time so that
the type of the argument is known. A data-dependency graph is built
for propagating the type of each value.  Each node of the graph is a
function call (built-in operators, such as addition, are implicit
function calls). Knowledge of function signatures is encoded in a
registry.  Given the argument types, the type inferencer looks up the
corresponding entry for the function and gets the return type. If a
different return type is obtained for a value, the two types are
unified by coercion with preference to avoid information
loss. Coercion is only available to numeric types and certain built-in
types. The data-dependency graph can contain cycles due to loops, so
the type inferencer runs until a fixed-point is reached. If the type
inference process fails, all values are assigned a generic ``Python
object'' type.

\subsection{High-Level Optimizations}

With type annotation on each variable in the Numba IR, several high-level
optimizations are performed before lowering. These optimizations exploit
high-level knowledge of Python semantics.

\subsubsection{Deferred Loop Specialization}

Since loops are likely to be compute-intensive, Numba will extract the
loops from function compiled in \textit{object mode} into a new
function.  The new function acts like a Numba \textit{@jit} decorated
function and defers compilation until its call site is reached. This
provides a second chance for Numba to optimize any compute intensive
loop to run in nopython mode.

Detecting loops with CPython bytecode is very simple. The
\textit{SETUP\_LOOP} bytecode describes the span of each loop. Numba
can simply copy all bytecodes marked by the \textit{SETUP\-\_LOOP} into
a new function. The loop in the original function is replaced with a
function call to the extracted loop.

\subsubsection{Array Expression}

Array expressions are formed when built-in Python operators, such as
addition and exponentiation, are applied to ndarray objects. After
type inference, an optimization pass searches for any array expression
and rewrites them into a loop. This avoids allocating temporary arrays
for intermediate results when an array expression contains multiple
operations.  For example, consider this function:

\begin{lstlisting}
@jit
def axpy(a, x, y):
  return a * b + c
\end{lstlisting}

In the \textit{axpy} function that takes three ndarrays, the normal
Python/NumPy execution will allocate an array for the intermediate
result of multiplication and one for the final result from
addition. Numba will rewrite this expression into a loop over the
three arrays with just one allocated output array.  Each iteration
computes both the multiplication and addition for each element in the
array, ensuring that registers can be used to hold intermediate scalar
results.

\subsection{Lowering}

The lowering phase is straightforward.  At this point, we have type
information for all values in the Numba IR. Each operation is
translated to LLVM IR according to a implementation registry for each
function known to the compiler (both built-in and user-defined).  For
each Python function, two functions are emitted in LLVM: one for the
actual compiled function, and a wrapper that acts as a bridge
between the interpreted Python runtime and the compiled Numba
runtime. The wrapper unboxes Python objects into machine
representation for use as arguments by the compiled function.  The
returned values is boxed from the machine representation back to the
Python objects.


\subsection{Support for GPGPUs}

% TODO: talk about memory managemnt

\section{Using LLVM}

In this section, we describe our experience with LLVM.
LLVM provides a simple API into a high quality compiler back-end with
JIT support readily available. We can focus the development effort
on the front-end without immense knowledge of the processor instruction set.
Once we had a working CPU back-end, porting Numba to support CUDA and HSA GPUs
was not difficult, since we map directly to the execution model of the GPUs.
It is just a matter of exposing target specific intrinsics to the Python
language level.

LLVM is arguably the best library for compiler developers but there are still
some issues and potential improvement. In the rest of
this section, we share several challenges we encountered and our experience
in trying to workaround them.

\subsection{API Stability}

LLVM development is fast and its C++ API changes frequently.  In early
days of Numba development, we maintained a Python binding, called
llvmpy, to the C++ API and have tried to mirror the C++ interface in
Python, but this was found to be very difficult.  To simplify the
binding and to minimize its exposure to the LLVM C++ API, we adopted
the LLVM C API instead and added any missing C wrappers around parts
of the LLVM C++ API that were needed.  The new binding is called
llvmlite\footnote{\url{http://llvmlite.pydata.org/}}.  It includes a
pure Python implementation of the the LLVM C++ IRBuilder API and build
a string representation of the LLVM IR.

\subsection{Error Handling}

Error handling inside LLVM does not follow a consistent convention.
Some errors are ignored or left unpropagated, which can lead to hard crashes
later on \footnote{\url{https://llvm.org/bugs/show_bug.cgi?id=6701}}.  Other
errors are reported by the API as a status code.  Other errors yet
trigger an assert() in the C++ source code, and therefore create a
controlled process crash at runtime.  Crashes (either controlled or
unpredictable) do not allow the Python binding to report errors to the
user in the expected way, i.e. as Python exceptions.

Furthermore, since error conditions are not exercised as much by the test
suite, regressions can sometimes happen.
\footnote{\url{https://llvm.org/bugs/show_bug.cgi?id=22368}}

\subsection{Managing ABI}

LLVM does not provide an abstraction for handling \textit{application binary
interface}(ABI). It is left for the front-end to handle this architecture
specific detail.  While we understand that ABI can be part of a language
design, a facility for support basic C ABI inside LLVM would simplify most of the
ABI issues.

To handle the different ABIs of various architectures, including GPGPUs which
have more restrictive ABIs, Numba avoids using any aggregate types as function
arguments or return types in the low-level code.  Instead, all aggregates are
flattened to simple scalars, e.g. integers, float and pointers.

To handle Python exceptions in compiled code, Numba uses the return
value of the compiled function for error code. The actual return value
is passed via the first argument. This error code passing style is the
same as the CPython API.

\subsection{Cross Module Linkage}

The legacy JIT in LLVM supports lazy compilation and allows function-at-a-time
code generation.  However, the new MCJIT does not. With the legacy JIT replaced
by MCJIT, Numba must compile each function in a new LLVM module and finalize it
immediately to trigger code generation. If function \textit{foo} calls
\textit{bar}, we need to register the address of \textit{bar} to the execution
engine before the finalization of \textit{foo}; otherwise, the finalization of
\textit{foo} will fail due to unresolved reference to \textit{bar}.
This makes handling of mutual recursion difficult.

Separating each functions in different module also make inlining difficult.
We have to maintain reference to all LLVM modules of every compiled function
and perform cross-module linking manually. A problem for this is that
LLVM functions can be duplicated in many modules; thus, increasing the memory
use.

We will be investigating whether the new On Request Compilation JIT
alleviates some of these issues.

% http://llvm.cc/t/llvmdev-new-jit-api-orcjit/219
% Fortunately, the On-Request-Compilation (ORC) JIT is an attempt to bring back
% lost feature of the legacy JIT. (TODO: what else can I say?)

\subsection{Object Cache}

The MCJIT features callback hooks for
applications to implement a form of object caching.
Before compiling a module, the MCJIT execution engine
calls the user-provided getObject() method.  That method can either
return a pointer to an existing memory area, in which case that
memory area is treated as an object file containing the module's
generated code (for example a ELF file under Linux), or a null pointer,
in which case compilation goes on as usual.

Then, whenever the MCJIT execution engine has finished compiling a
module (specifically, this means the getObject() method has returned
a null pointer), it calls the notifyObjectCompiled() method providing
both the module reference and a pointer to the memory area containing
the module's object code.

This callback-based model works well for a trivial form of caching,
where an LLVM module's object code is cached as-is, without any
additional information\footnote{This is how object caching is demonstrated in
the official Kaleidoscope tutorial \cite{llvmblog:kaleidoscope}}.
However, more complex forms of
caching need to serialize additional information beside the object code
for the module.  In Numba, this ancillary information is two-fold.  First,
we need to store metadata (a timestamp, a version number,
and potentially other data) in order to decide at runtime whether the
cached module is fresh or stale.  Second, we also need to store non-LLVM
data that is necessary to call the module's functions to be called,
information which is a akin to a closure and is composed of arbitrarily
complex Python objects.

Both pieces of information are not known at the LLVM module layer;
they reside at a higher level of abstraction.  We therefore had to
turn the callback model upside-down by keeping the object code inside
some long-lived variables, in order to be able to save or load
the object code at the place and time where it is natural to do so.

For our use case, and presumably for other non-trivial JIT use cases,
it would have been better to rely on a simple imperative API, allowing
both to fetch a module's object code (when compiled), and to feed object
code to the execution engine for a module.

A callback-based API is essentially useful when events are produced from
the outside, and/or in an uncontrolled way: for example, GUI or network
events.  Here, the events are predictable by the application
itself (compilation is triggered deterministically by calls to the
execution engine API), therefore an imperative API would have been
well-suited to the task.

\section{Conclusions}

We presented Numba, a Python JIT compiler that focuses in numeric
performance, which is critical for many scientific application. It offers
deferred loop specialization, array expression rewrite and multiple back-end
support. Finally, we shared our experience and feedback in using LLVM.

%\end{document}  % This is where a 'short' article might terminate

%ACKNOWLEDGMENTS are optional
\section{Acknowledgments}
We would like to acknowledge the support of the DARPA XDATA program for this work.

%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{unsrt}  % more sensible order than abbr
\bibliography{cite}  % cite.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%


% That's all folks!
\end{document}
