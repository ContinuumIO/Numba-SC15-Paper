% THIS IS SIGPROC-SP.TEX - VERSION 3.1
% WORKS WITH V3.2SP OF ACM_PROC_ARTICLE-SP.CLS
% APRIL 2009
%
% It is an example file showing how to use the 'acm_proc_article-sp.cls' V3.2SP
% LaTeX2e document class file for Conference Proceedings submissions.
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V3.2SP) *DOES NOT* produce:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) Page numbering
% ---------------------------------------------------------------------------------------------------------------
% It is an example which *does* use the .bib file (from which the .bbl file
% is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission,
% you need to 'insert'  your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% Questions regarding SIGS should be sent to
% Adrienne Griscti ---> griscti@acm.org
%
% Questions/suggestions regarding the guidelines, .tex and .cls files, etc. to
% Gerald Murray ---> murray@hq.acm.org
%
% For tracking purposes - this is V3.1SP - APRIL 2009

\documentclass{acm_proc_article-sp}

\begin{document}

\title{Numba: A LLVM-based Python JIT}
% \subtitle{[Extended Abstract]
%\titlenote{A full version of this paper is available as
%\textit{Author's Guide to Preparing ACM SIG Proceedings Using
%\LaTeX$2_\epsilon$\ and BibTeX} at
%\texttt{www.acm.org/eaddress.htm}}}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{2}

%  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Siu Kwan Lam\\
    \affaddr{Continuum Analytics}\\
    \affaddr{Austin, Texas}\\
    \email{siu@continuum.io}
% 2nd. author
\alignauthor
Antoine Pitrou\\
    \affaddr{Continuum Analytics}\\
    \affaddr{Austin, Texas}\\
    \email{antoine.pitrou@continuum.io}
}

\date{3 Sept 2015}

% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}
Dynamic, interpreted languages is attractive for domain-experts and scientist
for experimenting with new ideas.  However, the performance of the interpreter
is often a barrier for scaling a prototype into operating on large dataset.
This paper presents a just-in-time compiler for Python that focuses in
scientific computing. Starting with the simple syntax of Python, Numba compiles
a subset of the language into efficient machine code that is comparable to
traditional compiled language. In addition, we share our experience in building
a JIT compiler using LLVM.
\end{abstract}

% A category with the (minimum) three required fields
\category{D.3.4}{Programming Languages}{Processors}[Compilers, Optimization]
%A category including the fourth, optional field follows...
%\category{I.2.5}{Porgramming Languages and Software}{Metrics}[complexity measures, performance measures]

\terms{Languages, Performance}

\keywords{LLVM, Python, Compiler} % NOT required for Proceedings

\section{Introduction}

Python is a dynamically typed, interpreted language.
It is regarded as a high productivity language due to its simple syntax,
flexible semantic and the large number of libraries.
It has gained popularity in the scientific computing community,
which has driven the demand of high performance tools.
Among the many performance focused librarys, NumPy is one of the important
scientific library in the Python ecosystem as it provides a multi-dimensional
array (ndarray) object that became the foundation of efficient numeric
computation in Python.

A ndarray is a homogenous typed data memory buffer.
Data can be stored in either C contiguous, FORTRAN contiguous or have arbitrary
strides at each dimension.  This allows binding to high performance computing
libraries that are traditionally written in C or FORTRAN, such as BLAS and
LAPACK. The ndarray support array expression: using Python operators on
ndarrays will trigger element-wise operations that are implemented in C
efficiently.  However, a trivial looping of the elements in Python is
inefficient due to the layers of indirection inside interpreted code.

Numba \cite{numba:numba} is initially developed to optimize the inefficient
use-cases of NumPy.
The layers of indirection for ndarray indexing are flattened into direct
loading from pointers.  Before Numba, NumPy users had to write Python
C extensions to implement any custom computation in an efficient way.
The process can be error-prone due to manual handling of reference counting
of Python objects, and generally requires a lot of boilerplate code even
for simple use cases.  Numba lets users annotate a compute-intensive Python
function for compilation without rewriting the code in a low-level language.

\section{JIT for Numeric Python}

Numba is a function-at-a-time Just-in-Time compiler for CPython
\cite{wikipedia:cpython}, the most popular implementation of Python which,
as the name suggests, is written C.
It exists as a library to CPython and does not replace the interpeter.
Its initial focus is to target a Python subset that makes heavy use of
ndarrays in loops, so that users no longer need to rewrite their Python code in
low-level languages for better performance.
The ndarray provides important dimensionality, type and data layout information
that allows Numba to generate specialized  loops for arrays in machine code.
By knowing the native structure of ndarray,
it bypasses unnecessary indirection for accessing data in ndarrays.
The basic structure of the ndarray consists of the data pointer to the base of
the memory buffer and two integer arrays for the dimensionality (aka shape) and
the strides. Numba can directly access these fields for calculating the offset
of each element.  As a result, it can generate efficient loops that indexes
into ndarrays with performance similar to their counterpart written in compiled
language. As it continues to develop, the support language
subset is expanding. Currently, it can handle high-level features such as
generators and array expressions.

Unlike most Just-in-Time compilers for interpreted languages, Numba does not
perform tracing nor replace the interpreter.
Instead, it relies on user actively replacing the functions that need compiling.
In Python, this is done by applying a decorator to the function.
The decorator replaces the original Python function with a special object
that just-in-time compiles the function when being called.
To relieve user from the burden of type annotation, Numba inspect the types
of the arguments for local type inference on the function.
As a result, it can have accurate type information for each values without
tracing the execution.

The semantic of the compiled code slightly differs
from the interpreted. Numba supports a subset of the language only
and violates some semantic of Python.  It does not support big integers
and is limited to 64-bit integer for the maximum representable integer type.
All arithmetic operations on 64-bit integer wraparound when the result
overflow.  Variable type are not polymorphic.  Implicit coercion occurs when
a type is assigned to a variable of a different type. In practice, these
restrictions and deviations from Python semantic rarely affect the user
because the ndarray already has similar limitations. On the other hand, these
limitations allow for aggressive optimization.  As Numba continues to develop,
we aim to remove these limitions, but leave an option for user permit the
violation of Python semantic for optimization reason.

A novelty of Numba is its multi-target backend.  It currently supports NVIDIA
CUDA backend by using NVVM.  Support for AMD HSA is also available on APU by
using HLC.  Both NVVM and HLC are vendor-specific version of LLVM for
additional support for their hardware.  Multi-target support is an important
reason for the restricting the supported language.
Numba maps directly to the execution model of the GPGPU architectures.
Thus, it inherits all limitations.
Highly divergent code runs inefficiently on GPGPUs due to thread divergence.
Executing certain GPGPU specific constructs, like a thread barrier, by diverged
threads results in undefined behavior. Forcing each variable to be single-typed
avoids implicit branching that are hidden from user for fine-tuning the
performance and to avoid undefined behavior.

\section{Other Just-in-Time compilers for Python}

There are or were several other efforts to build Just-in-Time compilers
for Python.  Closest to Numba perhaps was Psyco, an opt-in compiler layered
on the CPython interpreter.  Psyco featured its own x86-only code generator
and was discontinued by its author, Armin Rigo, because its architecture
made maintenance and evolution difficult.  \cite{wikipedia:psyco}
\cite{rigo2004representation}

Another related effort was Unladen Swallow, a project initiated by some
Google engineers to produce an evolution of the CPython interpreter augmented
with a LLVM-based Just-in-Time compiler.  \cite{wikipedia:unladen_swallow}
The objective of Unladen Swallow was to support the full range of existing
Python code, and to make the generated code progressively faster by adding
"proven" optimizations.  The project was discontinued before it got
significant results for reasons which were explained by one of its authors.
\cite{kleckner:unladen_swallow_post_mortem}

Other Python Just-in-Time compilers have avoided building on CPython,
either re-using a significant runtime or building their own.  Of the
two main alive projects today, one (PyPy \cite{pypy:pypy}) has built its
own infrastructure from scratch, the other (Pyston \cite{github:pyston})
is using LLVM.  Both claim or intend to support the full range of features
Python offers.

Compared to those, Numba is much more opportunistic and opinionated.
It focusses on a narrow subset of Python's usecases on which it
aims to extract extremely good performance, comparable to what C or Fortran
code would achieve (or even better, when running on the GPU).


\section{Implementation}

Numba relies on user annotation by using a Python decorator (@jit) on functions.
The decorator subsitute the function object with a special object that triggers
the compilation when called. When a decorated function is called,
the call arguments are inspected. If the set of argument types have not appeared
before, Numba compile will compile a specialized version of the function for the
given types. Otherwise, the previously compiled version is called.

The following example illustrate the usage of the @jit decorator on a function:


\begin{lstlisting}
from numba import jit

@jit
def foo(a):
  return a + a
\end{lstlisting}

The compilation starts by converting the Python bytecode into the an
intermediate representation (IR) on which the type inference
is performed. If the type of each value in the IR can be inferred, the IR is
lowered to LLVM, which then emits the final machine code.  We call this the
\textit{nopython mode} because all operations can be lowered into efficient
machine code without relying on the Python runtime for any operation.
A \textit{nopython mode} function can be executed without the \textit{global
interpreter lock} (GIL) and is able to run in parallel threads.
If Numba cannot determine one of the value in the IR,
it assumes to all values in the function to be a Python object. At this point,
Numba must use the Python C-API and rely on the Python runtime for the
execution. The generated code would be equivalent to unrolling the interpreter
loop; thus, removeing the interpreter overhead.
This compilation mode serves as a fallback if Numba cannot infer the type
We call this the textit{object mode}.
of a value or function.

\subsection{Bytecode}

When calling a function in normal CPython execution, the interpreter creates
a new frame for the function and executes the function bytecode.
The bytecode is an instruction stream similar to x86 assembly.
Instructions are variable length. Branches are encoded as absolute or relative
jump instructions. Some bytecode instructions perform multiple tasks.
For instance, the \textit{JUMP\_IF\_TRUE\_OR\_POP} jump to the
target address if the value on the top-of-stack (TOS) evalutes to true;
otherwise, pop the TOS value.  Another complex instruction is the FOR\_ITER
instruction.  It is used in the encoding of the for-loop construct.
This instruction asks for the next value of the iterator at TOS.
If the iterator is exhausted, it pops the stack and jumps to end of loop
indicated by the operand of the instruction.  Otherwise, the next value of the
iterator is pushed on to the stack and proceed to the next instruction, which
is the first instruction of the loop body. Both of these instructions have
changes the control-flow and have optional stack-effect. \cite{pythondoc:dis}
Instructions like these are common and they cannot be mapped directly to
the low-level representation used by LLVM IR.

\subsection{Disassembling the bytecode}

Translation of the bytecode to LLVM IR is not trivial due to the
complexity of many common bytecode instructions.  We need to disassemble
the bytecode to recover the basic-blocks and to convert the stack machine into
a register machine to facilitate further analysis and lowering.

The first step of disassembling the bytecode is to recover the control flow
information. The bytecode is scanned for jump targets. The jump targets
indicate the start of the basic-blocks.  The jump instructions marks the
end of basic-blocks. This can be a simple process but, as mentioned in previous
section, some bytecode instructions can perform  operations on one of the
branch target.  Heuristics are used to reconstruct the basic-blocks properly
As a result, our bytecode disassembler is tailored to the specific behavior of
a CPython version.  Different version of CPython can change both the bytecode
instruction set and the way certain syntactic constructs are encoded.

Once all instructions are grouped into basic-blocks. We can simulate stack
operations to assign virtual register to each value.  Each basic-block is
simulated individually. Therefore, it is possible that the stack is empty when
a pop is encountered. In that case, a \textit{phi} node is inserted.
Its incoming values is later connected to the lingering stack value from the
incoming basic blocks.

\subsection{Internal Representation}

With the control-flow information and the stack-to-register mapping
of the bytecode, the bytecode is translated into an internal represenation,
called the Numba IR. The Numba IR is a higher-level representation of the
function logic than the bytecode. It captures the control-flow as basic-blocks
and values in variables.

During the translation to the Numba IR, a notion of scoping is introduce to
minimize the effect of monotyping for variables. An assignment to a variable
is considered a new variable definition when the definition is visible by all
subsequent basic-blocks. Otherwise, the assignment stores the new value to
the previous definition.  For example:

\begin{lstlisting}
@jit
def foo():
  a = 1
  bar(a)
  a = a + 2.5
  bar(a)
  a = a + 2j
  bar(a)
  return a
\end{lstlisting}

The three assignment to \textit{a} will create three definitions that have
types integer, float and complex, respectively.  As a result, the three
calls to \textit{bar} will be calling a different overloaded version.

\subsection{Type Inference}

Local type inference is applied to the Numba-IR at call time so that
the type of the argument is known. A data-dependency graph is built for
propagating the type of each value.  Each node of the graph is an function call
(builtin operators are implicit function calls). Knowledge of function
signatures is encoded in a registry.
Given the argument types, the type inferencer looks up
the corresponding entry for the function and gets the return type. If a
different return type is obtained for a value, the two types are unified by
coercion with preference to avoid information loss. Coercion is only available
to numeric types and certain built-in types. The data-dependency graph can
contain cycles due to loops. The type inferencer runs until a fixed-point is
reached. If the type inference fails, all values are assigned with a generic
Python object type.

\subsection{High-Level Optimizations}

With type annotation on each variable in the Numba IR, several high-level
optimizations are performed before lowering. These optimizations exploits
high-level knowledge of the Python semantic.

\subsubsection{Deferred Loop Specialization}

Since loops are likely to be compute-intensive, Numba will extract the loops
out from function compiled in \textit{object mode} into a new function.
The new function acts like a Numba \textit{@jit} decorated function and
waits for callsite compilation. This provides a second chance for Numba to
optimize any compute intensive loop.

Detecting loop with CPython bytecode is effortless. The \textit{SETUP\_LOOP}
bytecode describes the span of each loop. Numba can simply copy all
bytecodes marked by the \textit{SETUP\_LOOP} into a new function. The loop in
the original function is replaced with a function call to the extracted loop.

\subsubsection{Array Expression}

Array expressions are formed by applying built-in Python operators on
ndarray objects. After type inference, an optimization pass search for any
array expression and rewrite them into a loop. This avoids allocating
temporary arrays for intermedate results when an array expression contains
multiple operations.  For example:

\begin{lstlisting}
@jit
def axpy(a, x, y):
  return a * b + c
\end{lstlisting}

In the \textit{axpy} function that takes three ndarrays, the normal Python
execution will allocate an array for the multiplication and one for the
addition. Numba will rewrite it in a loop over the three arrays with just
one allocated output array.  Each iteration computes both the multiplication
and addition for each element in the array.

\subsection{Lowering}

The lowering phase is straightforward.  At this point, we have type information
for all values in the Numba IR. Each operation is translated to LLVM IR
according to a implementation registry for each functions.  For each Python
function, two functions are emitted in LLVM: one for the actual
actual the compiled function, and one for the bridge between the interpreted
runtime and the compiled runtime. The bridge unboxes Python objects into
machine representation for use as arguments by the compiled function.  The
returned values is boxed from the machine representation back to the Python
objects.


\section{Using LLVM}

In this section, we describe our experience with LLVM.
LLVM provides a simple API into a high quality compiler backend with
a JIT support readily available. We can focus the development effort
on the frontend without immense knowledge of the processor instruction set.
Once we have a working CPU backend, porting Numba to support CUDA and HSA GPUs
are not difficult.

LLVM is arguably the best library for compiler developers but there are still
some issues and potential improvement. In the rest of
this section, we share several challanges we encountered and our experience
in trying to workaround them.

\subsection{API Stability}

LLVM development is fast and its C++ API changes frequently.
In early days of Numba development, we maintained a Python binding,
called llvmpy, to the C++ API and have tried to keep a binding that
mirrors the C++ interface, but this was soon proven to be difficult.
To simplify the binding and to minimize its exposure to the LLVM C++ API,
we adopted the C API instead and added any missing interface to the C++ API.
The new binding is called llvmlite \cite{rtd:llvmlite}.  It mimicks the
C++ IRBuilder API in pure Python and builds up a string of the LLVM IR.

\subsection{Error Handling}

Error handling inside LLVM doesn't follow a consistent convention,
and seems mostly a best-effort concern.  Some errors are ignored or
left unpropagated, which can lead to hard crashes later on
\cite{llvmbugs:selinux}. Other errors are reported by the API as a status
code.  Other errors yet trigger an assert() in the C++ source code, and
therefore a controlled process crash at runtime.  Crashes (either
controlled or unpredictable) do not allow the Python binding to report
errors in the expected way, i.e. as Python exceptions.

Furthermore, since error conditions are not much exercised by the test
suite, regressions can sometimes happen. \cite{llvmbugs:llvmparsebitcode}

\subsection{Managing ABI}

LLVM does not provide an abstraction for handling \textit{application binary
interface}(ABI). It is left for the frontend to handle this architecture
specific detail.  While we understand that ABI can be part of a lanuguage
design, a facility for support basic C ABI inside LLVM can simplify most of the
ABI issue.

To handle the different ABI of multiple arhitectures, including GPGPUs which
has a more restrictive ABI, Numba avoids using any aggregate types as function
arguments or return type in the low-level code.  Instead, all aggregates are
flattened to simple scalars, e.g. intergers, float and pointers.

To handle Python exceptions in compiled code, Numba uses the return value of
the compiled function for error code. The actual return value is passed
in the first argument. This error code style is the same as the CPython API.

\subsection{Cross Module Linkage}

The legacy JIT in LLVM supports lazy compilation and allows function-at-a-time
code generation.  However, the new MCJIT does not. With the legacy replaced
by MCJIT, Numba must compile each function in a new LLVM module and finalize it
immediately to trigger code generation. If function \textit{foo} calls
\textit{bar}, we need to register the address of \textit{bar} to the execution
engine before the finalization of \textit{foo}; otherwise, the finalization of
\textit{foo} will fail due to unresolved reference to \textit{bar}.
This makes handling of mutual recursion difficult.

Separating each functions in different module also make inlining difficult.
We have to maintain reference to all LLVM modules of every compiled function
and perform cross-module linking manually. A problem for this is that
LLVM functions can be duplicated in many modules; thus, increasing the memory
use.

% http://llvm.cc/t/llvmdev-new-jit-api-orcjit/219
% Fortunately, the On-Request-Compilation (ORC) JIT is an attempt to bring back
% lost feature of the legacy JIT. (TODO: what else can I say?)

\subsection{Object Cache}

The MCJIT features callback hooks for
applications to implement a form of object caching.  It works
as follows \cite{llvmdoc:objectcache}.
Before compiling a module, the MCJIT execution engine
calls the user-provided getObject() method.  That method can either
return a pointer to an existing memory area, in which case that
memory area is treated as an object file containing the module's
generated code (for example a ELF file under Linux), or a null pointer,
in which case compilation goes on as usual.

Then, whenever the MCJIT execution engine has finished compiling a
module (specifically, this means the getObject() method has returned
a null pointer), it calls the notifyObjectCompiled() method providing
both the module reference and a pointer to the memory area containing
the module's object code.

This callback-based model works well for a trivial form of caching,
where an LLVM module's object code is cached as-is, without any
additional information (this is how object caching is demonstrated in
the official Kaleidoscope tutorial \cite{llvmblog:kaleidoscope}.
However, more complex forms of
caching need to serialize additional information beside the object code
for the module.  In Numba, this ancillary information is two-fold.  First,
we need to store freshness information (a timestamp, a version number,
and potentially other data) in order to decide at runtime whether the
cached module is fresh or stale.  Second, we also need to store non-LLVM
data that is necessary to call the module's functions to be called,
information which is a akin to a closure and is composed of arbitrarily
complex Python objects.

Both pieces of information are not known at the LLVM module layer;
they reside at a higher level of abstraction.  We therefore had to
turn the callback model upside-down by keeping the object code inside
some long-lived variables, in order to be able to save or load
the object code at the place and time where it is natural to do so.

For our use case, and presumably for other non-trivial JIT use cases,
it would have been better to rely on a simple imperative API, allowing
both to fetch a module's object code (when compiled), and to feed object
code to the execution engine for a module.

A callback-based API is essentially useful when events are produced from
the outside, and/or in an uncontrolled way: for example, GUI or network
events.  Here, the events are predictable by the application
itself (compilation is triggered deterministically by calls to the
execution engine API), therefore an imperative API would have been as
well suited to the task.

\section{Conclusions}

We presented Numba, a Python JIT compiler that focuses in numeric
performance, which is critical for many scientific application. It offers
deferred loop specialization, array expression rewrite and multiple backend
support. Finally, we shared our experience and feedback in using LLVM.

%\end{document}  % This is where a 'short' article might terminate

%ACKNOWLEDGMENTS are optional
% \section{Acknowledgments}
% Optional section

%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{cite}  % cite.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%


% That's all folks!
\end{document}
