% THIS IS SIGPROC-SP.TEX - VERSION 3.1
% WORKS WITH V3.2SP OF ACM_PROC_ARTICLE-SP.CLS
% APRIL 2009
%
% It is an example file showing how to use the 'acm_proc_article-sp.cls' V3.2SP
% LaTeX2e document class file for Conference Proceedings submissions.
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V3.2SP) *DOES NOT* produce:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) Page numbering
% ---------------------------------------------------------------------------------------------------------------
% It is an example which *does* use the .bib file (from which the .bbl file
% is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission,
% you need to 'insert'  your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% Questions regarding SIGS should be sent to
% Adrienne Griscti ---> griscti@acm.org
%
% Questions/suggestions regarding the guidelines, .tex and .cls files, etc. to
% Gerald Murray ---> murray@hq.acm.org
%
% For tracking purposes - this is V3.1SP - APRIL 2009

\documentclass{acm_proc_article-sp}

\begin{document}

\title{Numba: LLVM Practitioner Report}
% \subtitle{[Extended Abstract]
%\titlenote{A full version of this paper is available as
%\textit{Author's Guide to Preparing ACM SIG Proceedings Using
%\LaTeX$2_\epsilon$\ and BibTeX} at
%\texttt{www.acm.org/eaddress.htm}}}
%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{8} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Siu Kwan Lam\\
       \affaddr{Continuum Analytics}\\
       \affaddr{Somewhere}\\
       \affaddr{Austin, Texas}\\
       \email{siu@continuum.io}
% 2nd. author
\alignauthor
Add Your Name\\
       \affaddr{Institute for Clarity in Documentation}\\
       \affaddr{P.O. Box 1212}\\
       \affaddr{Dublin, Ohio 43017-6221}\\
       \email{webmaster@marysville-ohio.com}
% 3rd. author
\alignauthor
Add Your Name\\
       \affaddr{The Th{\o}rv{\"a}ld Group}\\
       \affaddr{1 Th{\o}rv{\"a}ld Circle}\\
       \affaddr{Hekla, Iceland}\\
       \email{larst@affiliation.org}
\and  % use '\and' if you need 'another row' of author names
% 4th. author
\alignauthor Add Your Name\\
       \affaddr{Brookhaven Laboratories}\\
       \affaddr{Brookhaven National Lab}\\
       \affaddr{P.O. Box 5000}\\
       \email{lleipuner@researchlabs.org}
% 5th. author
\alignauthor Add Your Name\\
       \affaddr{NASA Ames Research Center}\\
       \affaddr{Moffett Field}\\
       \affaddr{California 94035}\\
       \email{fogartys@amesres.org}
% 6th. author
\alignauthor Add Your Name\\
       \affaddr{Palmer Research Laboratories}\\
       \affaddr{8600 Datapoint Drive}\\
       \affaddr{San Antonio, Texas 78229}\\
       \email{cpalmer@prl.com}
}
% There's nothing stopping you putting the seventh, eighth, etc.
% author on the opening page (as the 'third row') but we ask,
% for aesthetic reasons that you place these 'additional authors'
% in the \additional authors block, viz.
\additionalauthors{Additional authors: John Smith (The Th{\o}rv{\"a}ld Group,
email: {\texttt{jsmith@affiliation.org}}) and Julius P.~Kumquat
(The Kumquat Consortium, email: {\texttt{jpkumquat@consortium.net}}).}
\date{30 July 1999}
% Just remember to make sure that the TOTAL number of authors
% is the number that will appear on the first page PLUS the
% number that will appear in the \additionalauthors section.

\maketitle
\begin{abstract}
This is the abstract.
\end{abstract}

% A category with the (minimum) three required fields
\category{H.4}{TODO}{Miscellaneous}
%A category including the fourth, optional field follows...
\category{D.2.8}{TODO}{Metrics}[complexity measures, performance measures]

\terms{Theory}

\keywords{LLVM, Python, Compiler} % NOT required for Proceedings

\section{Introduction}
TODO

\section{Background}

Python is a dynamically typed, interpreted language.
It is regarded as a high productivity language due to its simple syntax,
flexible semantic and the large number of libraries.
It has gained popularity in the scientific computing community,
which has driven the demand of high performance tools.
Among the many performance focused librarys, NumPy is one of the important
scientific library in the Python ecosystem as it provides a multi-dimensional
array (ndarray) object that became the foundation of efficient numeric
computation in Python.

A ndarray is a homogenous typed data memory buffer.
Data can be stored in either C contiguous, FORTRAN contiguous or have arbitrary
strides at each dimension.  This allows binding to high performance computing
libraries that are traditionally written in C or FORTRAN, such as BLAS and
LAPACK. The ndarray support array expression: using Python operators on
ndarrays will trigger element-wise operations that are implemented in C
efficiently.  However, a trivial looping of the elements in Python is
inefficient due to the layers of indirection inside interpreted code.

Numba is initially developed to optimize the inefficient use-cases of NumPy.
The layers of indirection for ndarray indexing is flattend into direct loading
from pointers.  Before Numba, NumPy users must write Python C-extension to
implement any custom computation.  The process can be error-prone due to manual
handling of reference counting of Python objects. Numba lets user annotate
the compute-intensive Python function for compilation without changing rewriting
the code.

\section{What is Numba?}

Numba is a function-at-a-time Just-in-Time compiler for CPython
\cite{wikipedia:cpython}, the most popular implementation of Python which,
as the name suggests, is written C.
It exists as a library to CPython and does not replace the interpeter.
Its initial focus is to target a Python subset that makes heavy use of
ndarrays in loops, so that users no longer need to rewrite their Python code in
low-level languages for better performance.
The ndarray provides important dimensionality, type and data layout information
that allows Numba to generate specialized  loops for arrays in machine code.
By knowing the native structure of ndarray,
it bypasses unnecessary indirection for accessing data in ndarrays.
The basic structure of the ndarray consists of the data pointer to the base of
the memory buffer and two integer arrays for the dimensionality (aka shape) and
the strides. Numba can directly access these fields for calculating the offset
of each element.  As a result, it can generate efficient loops that indexes
into ndarrays that are as fast as their FORTRAN counterpart
(TODO: benchmark for proof). As it continues to develop, the support language
subset is expanding. Currently, it can handle high-level features such as
generators and array expressions.

Unlike most Just-in-Time compilers for interpreted languages, Numba does not
perform tracing nor replace the interpreter.
Instead, it relies on user actively replacing the functions that need compiling.
In Python, this is done by applying a decorator to the function.
The decorator replaces the original Python function with a special object
that just-in-time compiles the function when being called.
To relieve user from the burden of type annotation, Numba inspect the types
of the arguments for local type inference on the function.
As a result, it can have accurate type information for each values without
tracing the execution.

The semantic of the compiled code slightly differs
from the interpreted. Numba supports a subset of the language only
and violates some semantic of Python.  It does not support big integers
and is limited to 64-bit integer for the maximum representable integer type.
All arithmetic operations on 64-bit integer wraparound when the result
overflow.  Variable type are not polymorphic.  Implicit coercion occurs when
a type is assigned to a variable of a different type. In practice, these
restrictions and deviations from Python semantic rarely affect the user
because the ndarray already has similar limitations. On the other hand, these
limitations allow for aggressive optimization.  As Numba continues to develop,
we aim to remove these limitions, but leave an option for user permit the
violation of Python semantic for optimization reason.

A novelty of Numba is its multi-target backend.  It currently supports NVIDIA
CUDA backend by using NVVM.  Support for AMD HSA is also available on APU by
using HLC.  Both NVVM and HLC are vendor-specific version of LLVM for
additional support for their hardware.  Multi-target support is an important
reason for the restricting the supported language.
Numba maps directly to the execution model of the GPGPU architectures.
Thus, it inherits all limitations.
Highly divergent code runs inefficiently on GPGPUs due to thread divergence.
Executing certain GPGPU specific constructs, like a thread barrier, by diverged
threads results in undefined behavior. Forcing each variable to be single-typed
avoids implicit branching that are hidden from user for fine-tuning the
performance and to avoid undefined behavior.

\section{Other Just-in-Time compilers for Python}

There are or were several other efforts to build Just-in-Time compilers
for Python.  Closest to Numba perhaps was Psyco, an opt-in compiler layered
on the CPython interpreter.  Psyco featured its own x86-only code generator
and was discontinued by its author, Armin Rigo, because its architecture
made maintenance and evolution difficult.  \cite{wikipedia:psyco}

Another related effort was Unladen Swallow, a project initiated by some
Google engineers to produce an evolution of the CPython interpreter augmented
with a LLVM-based Just-in-Time compiler.  \cite{wikipedia:unladen_swallow}
The objective of Unladen Swallow was to support the full range of existing
Python code, and to make the generated code progressively faster by adding
"proven" optimizations.  The project was discontinued before it got
significant results for reasons which were explained by one of its authors.
\cite{kleckner:unladen_swallow_post_mortem}

Other Python Just-in-Time compilers have avoided building on CPython,
either re-using a significant runtime or building their own.  Of the
two main alive projects today, one (PyPy \cite{pypy:pypy}) has built its
own infrastructure from scratch, the other (Pyston \cite{github:pyston})
is using LLVM.  Both claim or intend to support the full range of features
Python offers.

Compared to those, Numba is much more opportunistic and opinionated.
It focusses on a narrow subset of Python's usecases on which it
aims to extract extremely good performance, comparable to what C or Fortran
code would achieve (or even better, when running on the GPU).


\section{How Numba Works?}

Numba relies on user annotation by using a Python decorator (@jit) on functions.
The decorator subsitute the function object with a special object that triggers
the compilation when called. When a decorated function is called,
the call arguments are inspected. If the set of argument types have not appeared
before, Numba compile will compile a specialized version of the function for the
given types. Otherwise, the previously compiled version is called.

The compilation starts by converting the Python bytecode into the an
intermediate representation (IR) on which the type inference
is performed. If the type of each value in the IR can be inferred, the IR is
lowered to LLVM, which then emits the final machine code.  We call this the
\textit{nopython mode} because all operations can be lowered into efficient
machine code without relying on the Python runtime for any operation.
A \textit{nopython mode} function can be executed without the \textit{global
interpreter lock} (GIL) and is able to run in parallel threads.
If Numba cannot determine one of the value in the IR,
it assumes to all values in the function to be a Python object. At this point,
Numba must use the Python C-API and rely on the Python runtime for the
execution. The generated code would be equivalent to unrolling the interpreter
loop; thus, removeing the interpreter overhead.
This compilation mode serves as a fallback if Numba cannot infer the type
We call this the textit{object mode}.
of a value or function.

\subsection{Bytecode}

When calling a function in normal CPython execution, the interpreter creates
a new frame for the function and executes the function bytecode.
The bytecode is an instruction stream similar to x86 assembly.
Instructions are variable length. Branches are encoded as absolute or relative
jump instructions. Some bytecode instructions perform multiple tasks.
For instance, the JUMP\_IF\_TRUE\_OR\_POP jump to the
target address if the value on the top-of-stack (TOS) evalutes to true;
otherwise, pop the TOS value.  Another complex instruction is the FOR\_ITER
instruction.  It is used in the encoding of the for-loop construct.
This instruction asks for the next value of the iterator at TOS.
If the iterator is exhausted, it pops the stacka and jumps to end of loop
indicated by the operand of the instruction.  Otherwise, the next value of the
iterator is pushed on to the stack and proceed to the next instruction, which
is the first instruction of the loop body. Both of these instructions have
changes the control-flow and have optional stack-effect. \cite{pythondoc:dis}
Instructions like these are common and they cannot be mapped directly to
the low-level representation used by LLVM IR.

\subsection{Disassembling the bytecode}

Translation of the bytecode to LLVM IR is not trivial due to the
complexity of many common bytecode instructions.  We need to disassemble
the bytecode to recover the basic-blocks and to convert the stack machine into
a register machine to facilitate further analysis and lowering.

The first step of disassembling the bytecode is to recover the control flow
information. The bytecode is scanned for jump targets. The jump targets
indicate the start of the basic-blocks.  The jump instructions marks the
end of basic-blocks.

Now that all instructions are grouped into basic-blocks. We can simulate stack
operations to assign virtual register to each value.
When a pop operation is encountered and the stack is empty.  A \textit{phi}
node is inserted. Its incoming values can be connected to the lingering stack
value from the direct predecessors.

\subsection{Internal Representation}

With the control-flow information and the stack-to-register mapping
of the bytecode, the bytecode is translated into an internal represenation,
called Numba IR. The Numba IR is a higher-level representation of the function
logic than the bytecode.  It captures the control-flow as basic-blocks and values
in variables.

\subsubsection{Scoping}
As part of the translation, a concept of variable scoping is introduced.
In Python semantic, all variables in a function, including the arguments,
are in the local scope.  There are no nested scope within a function.
To workaround the lack of variant type (or tagged union),
Numba introduces a concept of scope to allow different version of the same
variable to exist inside a function; thus, a variable of the same name can
have different types in different part of problem.  For example:

\begin{lstlisting}
def foo():
  a = 1
  bar(a)
  a = a + 2.5
  bar(a)
  return a
\end{lstlisting}

At the first assignment, \textit{a} is an integer. At the second assignment,
\textit{a} is evolved to be a float. If the second assignment does not create
a new version of \textit{a}, our type inference will coerce \textit{a} to be
float for the entire function.

(XXX: I need to fix the scoping)
Generalizing to consider the control-flow, each variable access will use
the version available at the dominator.

\subsection{Type Inference and Lowering}

Local type inference is applied to the Numba-IR at call time so that we
the type of the argument is known. A data-dependency graph is built for
propagating the type of each value.  Each node of the graph is an function call
(builtin operators are implicit function calls). Knowledge of function signature
is encoded in a registry. Given the argument types, the type inferencer look up
the corresponding entry for the function and gets the return type. If a
different return type is obtained for a value, the two type is unified by
coercion with preference to avoid information loss. Coercion is only available
to numeric types and certain built-in types. The data-dependency graph can
contain cycles due to loops. The type inferencer runs until a fixed-point is
reached. If the type inference fails, we assume all values are generic Python
objects.

The lowering phase is straightforward.  At this point, we have type information
for all values in the Numba IR. Each operation is translated to LLVM IR
according to a implementation registry for each functions.

\section{Optimizations}

\subsection{Type erasure}

In \textit{nopython mode}, type information is erased in the generated code.
For function that have overloaded signatures, the exact version to call is known
due to type inference. There are no runtime type-based dispatch in generated
code.

\subsection{Deferred Loop Specialization}

Since loops are likely to be compute-intensive, Numba will \"lift\" the loops
out from function compiled in \textit{object mode} into a new function.
The new function acts like a Numba \textit{@jit} decorated function and
waits for callsite compilation. This provides a second chance for Numba to
optimize any compute intensive loop.

\subsection{Array Expression}

Array expressions are formed by applying built-in Python operators on
ndarray objects. After type inference, an optimization pass search for any
array expression and rewrite them into a loop. This avoids allocating
temporary arrays for intermedate results.
(TODO: Need more details)

\section{Working with LLVM}

Here, we describe our experience in using LLVM.

\subsection{API stability}

LLVM development is fast and its C++ API changes frequently.
In early days of Numba development, we maintained a Python binding,
called llvmpy, to the C++ API and have tried to keep a binding that
mirrors the C++ interface, but this was soon proven to be difficult.
To simplify the binding and to minimize its exposure to the LLVM C++ API,
we adopted the C API instead and added any missing interface to the C++ API.
The new binding is called llvmlite \cite{rtd:llvmlite}.  It mimicks the
C++ IRBuilder API in pure Python and builds up a string of the LLVM IR.

(TODO: talk about C assertion in LLVM)

(TODO: Convey buffer contiguousness by GEP and not strides.
NumPy uses strides only.)

\subsection{Managing ABI}

LLVM does not provide abstraction for handling \textit{application binary
interface}(ABI). It is left for the frontend to handle this architecture
specific detail.  While we understand that ABI can be part of a lanuguage
design, we hope that there are facilities for support basic C ABI inside
LLVM.

To handle the different ABI of multiple arhitectures, including GPGPUs which
has a more restrictive ABI, Numba avoids using any aggregate types as function
arguments or return type in the low-level code.  Instead, all aggregates are
flattened to simple scalars, e.g. intergers, float and pointers.

\subsection{Slow code emission, OrcJIT}

(Does not seem to be dominating the Numba compilation pipeline)
(Maybe removing this section)

\subsection{Cross module linkage, OrcJIT}

The legacy JIT in LLVM supports lazy compilation and allows function-at-a-time
code generation.  However, the new MCJIT does not. As as result, Numba must
compile each function in a new LLVM module and finalize it immediately to
trigger code generation. If function \textit{foo} calls \textit{bar},
we need to register the address of \textit{bar} to the execution engine
before the finalization of \textit{foo}; otherwise, the finalization of
\textit{foo} will fail due to unresolved reference to \textit{bar}.
This makes handling of mutual recursion difficult.

Separating each functions in different module also make inlining difficult.
We have to maintain reference to all LLVM modules of every compiled function
and perform cross-module linking manually. A problem for this is duplicated
LLVM function in many modules and is memory expensive.

\subsection{Object Cache}

LLVM's MCJIT, which is used by Numba, features callback hooks for
applications to implement a form of object caching.  It works
as follows \cite{llvmdoc:objectcache}.
Before compiling a module, the MCJIT execution engine
calls the user-provided getObject() method.  That method can either
return a pointer to an existing memory area, in which case that
memory area is treated as an object file containing the module's
generated code (for example a ELF file under Linux), or a null pointer,
in which case compilation goes on as usual.

Then, whenever the MCJIT execution engine has finished compiling a
module (specifically, this means the getObject() method has returned
a null pointer), it calls the notifyObjectCompiled() method providing
both the module reference and a pointer to the memory area containing
the module's object code.

This callback-based model works well for a trivial form of caching,
where an LLVM module's object code is cached as-is, without any
additional information (this is how object caching is demonstrated in
the official Kaleidoscope tutorial \cite{llvmblog:kaleidoscope}.
However, more complex forms of
caching need to serialize additional information beside the object code
for the module.  In Numba, this ancillary information is two-fold.  First,
we need to store freshness information (a timestamp, a version number,
and potentially other data) in order to decide at runtime whether the
cached module is fresh or stale.  Second, we also need to store non-LLVM
data that is necessary to call the module's functions to be called,
information which is a akin to a closure and is composed of arbitrarily
complex Python objects.

Both pieces of information are not known at the LLVM module layer;
they reside at a higher level of abstraction.  We therefore had to
turn the callback model upside-down by keeping the object code inside
some long-lived variables, in order to be able to save or load
the object code at the place and time where it is natural to do so.

For our use case, and presumably for other non-trivial JIT use cases,
it would have been better to rely on a simple imperative API, allowing
both to fetch a module's object code (when compiled), and to feed object
code to the execution engine for a module.

A callback-based API is essentially useful when events are produced from
the outside, and/or in an uncontrolled way: for example, GUI or network
events.  Here, the events are predictable by the application
itself (compilation is triggered deterministically by calls to the
execution engine API), therefore an imperative API would have been as
well suited to the task.

\section{Conclusions}
My conclusion
%\end{document}  % This is where a 'short' article might terminate

%ACKNOWLEDGMENTS are optional
\section{Acknowledgments}
Optional section

%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{cite}  % cite.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%


% That's all folks!
\end{document}
